{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmaguero/diploma_fpuna_nlp_ia/blob/master/2025/chainlit_faiss_llm_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6EMFg6fTWei"
      },
      "source": [
        "# MEDICAL CHATBOT\n",
        "CHAINLIT+PDF+FAISS_DB+LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxPV0Q4zPW-"
      },
      "source": [
        "## Initial environment settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ-Om_qlzPW_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initial environment settings\n",
        "import os\n",
        "from pathlib import Path\n",
        "HOME = str(Path.home())\n",
        "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
        "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y865t7qczPXA"
      },
      "source": [
        "## Confirm your CUDA version and whether you can use a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdlvsZZTzPXA",
        "outputId": "8de7bdcd-f3b0-4d18-b323-c057a042ea98",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 26 15:31:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0             32W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4oZaGRMzPXB"
      },
      "source": [
        "## Installation kit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctLWSpJYzPXB",
        "outputId": "aeda7ac7-6d8b-40ef-848b-9c8afde6cce4",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.19.0 requires starlette<1.0.0,>=0.46.2, but you have starlette 0.41.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install cohere gdown>=4.7.3 kaleido langchain langchain-community langchain-text-splitters langchain-core langchain-openai chainlit pyngrok pypdf python-dotenv sentence-transformers tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate bitsandbytes faiss-cpu faiss-gpu hf_transfer huggingface_hub optimum transformers  -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb05YchxCci8",
        "outputId": "9af53b70-d550-45a0-d34b-c8424b564b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langgraph==0.2.74 langchain_core==0.3.37 langchain_openai==0.3.6 langchain==0.3.19 langchain_community==0.3.18 chainlit==2.5.5 --force-reinstall -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tfwzZNvtDwg",
        "outputId": "ff193b1f-ca01-4e43-90c6-951a59094ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/referencing/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/d8/b2/9bf8c07b18ef34dc70771a0e21f6f0a5392dd3a753135743178cd59f1b7f/chainlit-2.5.5-py3-none-any.whl\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gptqmodel 5.4.2 requires numpy==2.2.6, but you have numpy 2.3.5 which is incompatible.\n",
            "cohere 5.20.0 requires httpx-sse==0.4.0, but you have httpx-sse 0.4.3 which is incompatible.\n",
            "langchain-classic 1.0.0 requires langchain-core<2.0.0,>=1.0.0, but you have langchain-core 0.3.37 which is incompatible.\n",
            "langchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.3.6 which is incompatible.\n",
            "opentelemetry-instrumentation-agno 0.49.3 requires opentelemetry-instrumentation>=0.59b0, but you have opentelemetry-instrumentation 0.58b0 which is incompatible.\n",
            "opentelemetry-instrumentation-agno 0.49.3 requires opentelemetry-semantic-conventions>=0.59b0, but you have opentelemetry-semantic-conventions 0.58b0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.3.37 which is incompatible.\n",
            "google-adk 1.19.0 requires starlette<1.0.0,>=0.46.2, but you have starlette 0.41.3 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 46.0.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.10.0 which is incompatible.\n",
            "transformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.1.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbya0TkuCf4F",
        "outputId": "faa7a7a0-27e3-4de7-80a7-0c7a46827a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gptqmodel -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTI9qPnfp-Rq",
        "outputId": "620133d6-0652-4480-dcfe-256fac2a0938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69m9gq40x-_"
      },
      "source": [
        "### File download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8QjjxAyz5GJ",
        "outputId": "cb059a0e-9215-431e-ad16-0b56ca6c78ab",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pUDgs3YMnlr8See8Rld3L1ZRiTeeOlMM\n",
            "To: /content/data/book/Medical_Chatbot.pdf\n",
            "100% 16.1M/16.1M [00:00<00:00, 40.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p data/book/\n",
        "!gdown 1pUDgs3YMnlr8See8Rld3L1ZRiTeeOlMM -O data/book/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnqjWAd3zPXB"
      },
      "source": [
        "## Step1: create ingest.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNADmOT9zPXB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat << \\EOF >  ingest.py\n",
        "\n",
        "# RUN: python3 ingest.py\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "DATA_PATH = 'data/book/'\n",
        "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
        "\n",
        "# Create vector database\n",
        "def create_vector_db():\n",
        "    loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                                   chunk_overlap=100)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "      model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "      model_kwargs={'device': 'cuda'} # else cpu\n",
        "    )\n",
        "\n",
        "    db = FAISS.from_documents(texts, embeddings)\n",
        "    db.save_local(DB_FAISS_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_vector_db()\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vYQbgk07Us"
      },
      "source": [
        "### Vector storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ4Jv_z2hRh6",
        "outputId": "d061f1d9-67f8-4958-d640-af3f5eab460b",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ingest.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "2025-11-26 15:35:55.273027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764171355.309043   14584 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764171355.319143   14584 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764171355.345185   14584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764171355.345222   14584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764171355.345231   14584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764171355.345238   14584 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-26 15:35:55.352290: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n"
          ]
        }
      ],
      "source": [
        "# Vector storage to FAISS\n",
        "!python3 ingest.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHRUku5xzPXC",
        "tags": []
      },
      "source": [
        "## Step2: create model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjLyW3EGzPXC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat << \\EOF >  model.py\n",
        "# RUN: chainlit run model.py\n",
        "\n",
        "# 01: CONFIGURE MODEL_ID and DB PATHs\n",
        "#MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"\n",
        "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
        "\n",
        "# 02: Load LIBRARY\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
        "import chainlit as cl\n",
        "import torch\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import os\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "# 03: custom_prompt_template\n",
        "custom_prompt_template = \"\"\"\n",
        "You are a medical bot.\n",
        "Use the following pieces of information (Context) to answer the user's Question in Spanish.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else. Always answer in Spanish.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "def set_custom_prompt():\n",
        "    \"\"\"\n",
        "    Prompt template for QA retrieval for each vectorstore\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "# 04: Retrieval QA Chain\n",
        "def retrieval_qa_chain(llm, prompt, db):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type='stuff',\n",
        "                                       retriever=db.as_retriever(search_kwargs={'k': 5}),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={'prompt': prompt}\n",
        "                                       )\n",
        "    return qa_chain\n",
        "\n",
        "def load_llm():\n",
        "    # 04: Full LLM Model or GPTQ\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        top_k=40,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False,\n",
        "    )\n",
        "    llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.7})\n",
        "\n",
        "    return llm\n",
        "\n",
        "# 05: QA Model Function\n",
        "def qa_bot():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", model_kwargs={'device': 'cuda'})\n",
        "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "    llm = load_llm()\n",
        "    qa_prompt = set_custom_prompt()\n",
        "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
        "\n",
        "    return qa\n",
        "\n",
        "# 06: output function\n",
        "def final_result(query):\n",
        "    qa_result = qa_bot()\n",
        "    response = qa_result({'query': query})\n",
        "    return response\n",
        "\n",
        "# 07: chainlit code\n",
        "@cl.on_chat_start\n",
        "async def start():\n",
        "    chain = qa_bot()\n",
        "    msg = cl.Message(content=\"Starting the bot...\")\n",
        "    await msg.send()\n",
        "    msg.content = \"Hi, Welcome to Medical Bot. What is your query?\"\n",
        "    await msg.update()\n",
        "\n",
        "    cl.user_session.set(\"chain\", chain)\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: cl.Message):\n",
        "    chain = cl.user_session.get(\"chain\")\n",
        "    cb = cl.AsyncLangchainCallbackHandler(\n",
        "        stream_final_answer=True, answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
        "    )\n",
        "    cb.answer_reached = True\n",
        "    res = await chain.acall(message.content, callbacks=[cb])\n",
        "    answer = res[\"result\"]\n",
        "    sources = res[\"source_documents\"]\n",
        "    source_elements = []\n",
        "\n",
        "    found_sources = []\n",
        "\n",
        "    for i, doc in enumerate(sources):\n",
        "        page_content=(res['source_documents'][i].page_content)\n",
        "        page=(res['source_documents'][i].metadata[\"page\"])\n",
        "        source=res['source_documents'][i].metadata[\"source\"]\n",
        "        file = os.path.basename(source)\n",
        "        #print(\"SOURCE: \"+file+\", PAGE: \"+str(page) )\n",
        "        source_name=f\"source_{i}\"\n",
        "\n",
        "        found_sources.append(source_name)\n",
        "        text=\"[Document: \"+file+\", Page: \"+str(page)+\"]\\n\\nContent: \"+page_content\n",
        "        source_elements.append(cl.Text(content=text, name=source_name))\n",
        "\n",
        "    if sources:\n",
        "        #answer += f\"\\n\\n\\nSource of information, Sources:\" + str(sources)\n",
        "        answer += f\"\\nSources: {', '.join(found_sources)}\"\n",
        "    else:\n",
        "        answer += \"\\n\\n\\nNo sources found\"\n",
        "\n",
        "    #await cl.Message(content=answer).send()\n",
        "    await cl.Message(content=answer, elements=source_elements).send()\n",
        "\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO0zjj-SzPXD"
      },
      "source": [
        "## Execution Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "c8df33bd6c1745169ca1342bc485d27f",
            "63b88bd2a5034ed3825f910fe34f6180",
            "0e711b88208c4bff8a25e6920916cebe",
            "206b6c1dd5f9433382429ec453b6b0f4",
            "58e888ee8cf34893b0e8bc2c2745743a",
            "42b3dc5825864255a7d3cd14b5f3aaea",
            "b0b994e76c604baa879209cb9c4184ac",
            "08c21784499947b2a62b160a3fac1443",
            "b184aae589684ee0ac239d25a4d1114f",
            "937323a502b7442187d9005e8f8f32ee",
            "a2990c8fe524415ca265316c43e271f2",
            "edb9e94f01d7411da9fbe2181a8dda90",
            "a2d950ee466640008ea7fddb2b597358",
            "cfbd314ceadc49df9b40a24bc73278a8",
            "48907585d53c47d1b021ce183fb3319f",
            "60c5c0b686e047dda44892af0912c6d0",
            "3e1d3fd19b454d32afd9b2d18c6e2fd0",
            "8a34aa26bca44d338eb27a2399a825c1",
            "15a78b6ffe3542e0b2b54583aaf7ab80",
            "79fbb99d896249de87ca90744fa6cb8e"
          ]
        },
        "id": "QUO6xuXKsQU5",
        "outputId": "bfb8f5ea-7af3-4814-fc1c-19d89e5041a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8df33bd6c1745169ca1342bc485d27f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNhQSeFOzPXD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# CHAINLIT\n",
        "!chainlit run model.py -w &> /content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8000)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "collapsed": true,
        "id": "DY6HxPaNBvHs",
        "outputId": "94593b93-13e3-4b67-a369-26419647947e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://8000-gpu-t4-s-1zuzb2smm4xia-a.us-west4-2.prod.colab.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ngrok (optional)"
      ],
      "metadata": {
        "id": "fHCLnHqQN5y-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd7LoHR752rg"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken xxx\n",
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHoEEHrbzPXE",
        "tags": []
      },
      "source": [
        "### DELETE JOB, Execute again before the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6UTG3Bc0F85"
      },
      "outputs": [],
      "source": [
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfFTVIWKkhgt"
      },
      "outputs": [],
      "source": [
        "!ps -ef |grep chainlit | awk '{print $2}' | xargs kill -9\n",
        "!ps -ef |grep ngrok | awk '{print $2}' | xargs kill -9\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Image_S_work-genai3_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen",
      "language": "python",
      "name": "s_work-genai3_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8df33bd6c1745169ca1342bc485d27f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_b0b994e76c604baa879209cb9c4184ac"
          }
        },
        "63b88bd2a5034ed3825f910fe34f6180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08c21784499947b2a62b160a3fac1443",
            "placeholder": "​",
            "style": "IPY_MODEL_b184aae589684ee0ac239d25a4d1114f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0e711b88208c4bff8a25e6920916cebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_937323a502b7442187d9005e8f8f32ee",
            "placeholder": "​",
            "style": "IPY_MODEL_a2990c8fe524415ca265316c43e271f2",
            "value": ""
          }
        },
        "206b6c1dd5f9433382429ec453b6b0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_edb9e94f01d7411da9fbe2181a8dda90",
            "style": "IPY_MODEL_a2d950ee466640008ea7fddb2b597358",
            "value": true
          }
        },
        "58e888ee8cf34893b0e8bc2c2745743a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cfbd314ceadc49df9b40a24bc73278a8",
            "style": "IPY_MODEL_48907585d53c47d1b021ce183fb3319f",
            "tooltip": ""
          }
        },
        "42b3dc5825864255a7d3cd14b5f3aaea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60c5c0b686e047dda44892af0912c6d0",
            "placeholder": "​",
            "style": "IPY_MODEL_3e1d3fd19b454d32afd9b2d18c6e2fd0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b0b994e76c604baa879209cb9c4184ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "08c21784499947b2a62b160a3fac1443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b184aae589684ee0ac239d25a4d1114f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "937323a502b7442187d9005e8f8f32ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2990c8fe524415ca265316c43e271f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edb9e94f01d7411da9fbe2181a8dda90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2d950ee466640008ea7fddb2b597358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfbd314ceadc49df9b40a24bc73278a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48907585d53c47d1b021ce183fb3319f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "60c5c0b686e047dda44892af0912c6d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e1d3fd19b454d32afd9b2d18c6e2fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a34aa26bca44d338eb27a2399a825c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a78b6ffe3542e0b2b54583aaf7ab80",
            "placeholder": "​",
            "style": "IPY_MODEL_79fbb99d896249de87ca90744fa6cb8e",
            "value": "Connecting..."
          }
        },
        "15a78b6ffe3542e0b2b54583aaf7ab80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79fbb99d896249de87ca90744fa6cb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}